adapter_append: false
add_adapter: false
alpha: 0
always_freeze:
  text_encoder: []
  visual_encoder: []
batch_size: 128
bitfit: false
conventional_adapter: {insert: false, reduction_factor: 4}
disable_wandb: true
embed_dim: 256
fp16: false
freeze_proj: false
freeze_text_encoder: false
freeze_vision_encoder: false
image_res: 256
image_tokenizer_path: /net/acadia10a/data/zkhan/dall-e-tokenizer-weights
limit_num_samples: false
load_strict: true
model_config: {import_path: models.clip_adjustable.CLIP}
optimizer: {lr: 0.0001, opt: adamW, weight_decay: 0.02}
pretrained_text: true
pretrained_vision: true
schedular: {cooldown_epochs: 0, decay_rate: 1, epochs: 15, lr: 0.0001, min_lr: 1e-05,
  sched: cosine, warmup_epochs: 10, warmup_lr: 1e-05}
temp: 0.07
text_encoder: tiny
train_file: [/net/acadia10a/data/zkhan/coco2017/pretrain-pairs.json, /net/acadia10a/data/zkhan/visual-genome-sandbox/visual-genome-pairs.json,
  /net/acadia10a/data/zkhan/sbu-captions/sbu-pretrain-pairs.json, /net/acadia10a/data/zkhan/cc3m/cc3m-train-pairs.json,
  /net/acadia10a/data/zkhan/cc3m/cc3m-val-pairs.json]
unlock_attn: false
unlock_dense: false
unlock_layernorm: false
unlock_random: false
version: 2
vision_encoder: tiny
